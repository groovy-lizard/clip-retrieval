{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip Retrieval Client API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python module allows you to query a backend remote via its exposed REST api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazye/anaconda3/envs/clipenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from clip_retrieval.clip_client import ClipClient, Modality\n",
    "\n",
    "IMAGE_BASE_URL = \"https://github.com/rom1504/clip-retrieval/raw/main/test/test_clip_inference/test_images/\"\n",
    "\n",
    "def log_result(result):\n",
    "    id, caption, url, similarity = result[\"id\"], result[\"caption\"], result[\"url\"], result[\"similarity\"]\n",
    "    print(f\"id: {id}\")\n",
    "    print(f\"caption: {caption}\")\n",
    "    print(f\"url: {url}\")\n",
    "    print(f\"similarity: {similarity}\")\n",
    "    display(Image(url=url, unconfined=True))\n",
    "\n",
    "\n",
    "client = ClipClient(\n",
    "    url=\"https://knn.laion.ai/knn-service\",\n",
    "    indice_name=\"laion5B-L-14\",\n",
    "    aesthetic_score=9,\n",
    "    aesthetic_weight=0.5,\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 518836491\n",
      "caption: orange cat with supicious look stock photo\n",
      "url: https://media.istockphoto.com/photos/orange-cat-with-supicious-look-picture-id907595140?k=6&amp;m=907595140&amp;s=612x612&amp;w=0&amp;h=4CTvSxNvv4sxSCPxViryha4kAjuxDbrXM5vy4VPOuzk=\n",
      "similarity: 0.5591729879379272\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://media.istockphoto.com/photos/orange-cat-with-supicious-look-picture-id907595140?k=6&amp;m=907595140&amp;s=612x612&amp;w=0&amp;h=4CTvSxNvv4sxSCPxViryha4kAjuxDbrXM5vy4VPOuzk=\" class=\"unconfined\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_results = client.query(text=\"an image of a cat\")\n",
    "log_result(cat_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 574870177\n",
      "caption: Palm trees in Orlando, Florida\n",
      "url: https://www.polefitfreedom.com/wp-content/uploads/2018/03/Orlando.jpg\n",
      "similarity: 0.961936354637146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.polefitfreedom.com/wp-content/uploads/2018/03/Orlando.jpg\" class=\"unconfined\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beach_results = client.query(image=\"https://github.com/rom1504/clip-retrieval/raw/main/tests/test_clip_inference/test_images/321_421.jpg\")\n",
    "log_result(beach_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip  # pylint: disable=import-outside-toplevel\n",
    "import torch\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=\"cpu\", jit=True)\n",
    "\n",
    "import urllib\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def download_image(url):\n",
    "    urllib_request = urllib.request.Request(\n",
    "        url,\n",
    "        data=None,\n",
    "        headers={\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\"},\n",
    "    )\n",
    "    with urllib.request.urlopen(urllib_request, timeout=10) as r:\n",
    "        img_stream = io.BytesIO(r.read())\n",
    "    return img_stream\n",
    "\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(12, axis)\n",
    "\n",
    "\n",
    "def get_text_emb(text):\n",
    "    with torch.no_grad():\n",
    "        text_emb = model.encode_text(clip.tokenize([text], truncate=True).to(\"cpu\"))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
